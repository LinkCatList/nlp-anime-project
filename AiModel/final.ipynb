{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Установим библиотеки"
      ],
      "metadata": {
        "id": "wW0U2QcOaZgs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faFdBlgfWorc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "nltk.download('punkt')\n",
        "\n",
        "from keras.models import load_model\n",
        "from googletrans import Translator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Предобработка данных"
      ],
      "metadata": {
        "id": "q7vaTUjpamEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этом разделе мы подготовим наши данные к токенизации и прочим штукам\n",
        "\n",
        "Train dataset:"
      ],
      "metadata": {
        "id": "4Y-9uv5Tapk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# загрузка\n",
        "train_df = pd.read_csv('/content/train_new.csv')\n",
        "train_df = train_df.rename(columns={'Unnamed: 0': 'Id'})\n",
        "\n",
        "# создаем датафрейм с кол-во жанров в трейне\n",
        "genre_count_df = train_df[['Id','genres']].groupby('genres').agg('count').sort_values('Id', ascending = False)\n",
        "\n",
        "genre_count_df.reset_index(inplace = True)\n",
        "genre_count_df.rename(columns ={'Id':'genre_cnt'}, inplace = True)\n",
        "\n",
        "# рисуем график\n",
        "genre_count_df.plot.bar(y = 'genre_cnt', x = 'genres')\n",
        "\n",
        "# объединяем\n",
        "train_df = train_df.merge(genre_count_df, how = 'left', left_on='genres', right_on='genres')\n",
        "\n",
        "# убираем жанры, кол-во которых меньше min_cnt\n",
        "min_cnt = 700\n",
        "train_df = train_df[train_df['genre_cnt'] > min_cnt]\n",
        "train_df['genre_cnt'] = train_df['genre_cnt'].replace('nan', 0)"
      ],
      "metadata": {
        "id": "4O3mBES0alvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.genres.value_counts() # получаем 10 жанров, которые позже будем предсказывать"
      ],
      "metadata": {
        "id": "mrBL-sNSbCxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dataset:"
      ],
      "metadata": {
        "id": "milaVNHEbtBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# загрузка\n",
        "test_df =  pd.read_csv('/content/test_new.csv')\n",
        "test_df = test_df.rename(columns={'Unnamed: 0': 'Id'})\n",
        "\n",
        "# создаем датафрейм с кол-во жанров в тесте\n",
        "genre_count_df_test = test_df[['Id','genres']].groupby('genres').agg('count').sort_values('Id', ascending = False)\n",
        "\n",
        "genre_count_df_test.reset_index(inplace = True)\n",
        "genre_count_df_test.rename(columns ={'Id':'genre_cnt'}, inplace = True)\n",
        "\n",
        "# рисуем график\n",
        "genre_count_df_test.plot.bar(y = 'genre_cnt', x = 'genres')\n",
        "\n",
        "# объединяем\n",
        "test_df = test_df.merge(genre_count_df_test, how = 'left', left_on='genres', right_on='genres')\n",
        "\n",
        "# убираем жанры, кол-во которых меньше min_cnt\n",
        "min_cnt = 700\n",
        "test_df = test_df[test_df['genre_cnt'] > min_cnt]\n",
        "test_df['genre_cnt'] = test_df['genre_cnt'].replace('nan', 0)"
      ],
      "metadata": {
        "id": "Y28nObMsbsxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# уберем пропуски в столбцах description и genres\n",
        "\n",
        "train_df = train_df.dropna(subset=['description', 'genres'])\n",
        "test_df = test_df.dropna(subset=['description', 'genres'])"
      ],
      "metadata": {
        "id": "dwclRY5_cRBq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Токенизация"
      ],
      "metadata": {
        "id": "spRRRNaycDeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# создадим функцию, которая будет токенизировать текст\n",
        "def tokenize_text(text):\n",
        "  tokenized_text = nltk.word_tokenize(text)\n",
        "  tokens = [i.lower() for i in tokenized_text if ( i not in string.punctuation )]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "_rCfYbG3cK6x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# применяем к нашим данным\n",
        "train_df['description_tokenized'] = train_df['description'].apply(lambda x:tokenize_text(x))\n",
        "test_df['description_tokenized'] = test_df['description'].apply(lambda x:tokenize_text(x))"
      ],
      "metadata": {
        "id": "hN5Or4Vkcj6k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание словаря"
      ],
      "metadata": {
        "id": "YeU6Hn2ec2ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "mx = 1000000\n",
        "\n",
        "vocab[\"<PAD>\"] = mx + 2;\n",
        "vocab[\"<START>\"] = mx + 1;\n",
        "vocab[\"<UNKW>\"] = mx;\n",
        "\n",
        "for tokens in train_df['description_tokenized']:\n",
        "    for word in tokens:\n",
        "        if word not in vocab.keys():\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1\n",
        "\n",
        "vocab = {k: v for k, v in sorted(vocab.items(), key = lambda item: item[1], reverse = True)}\n",
        "\n",
        "cnt = 0\n",
        "for k in vocab.keys():\n",
        "  vocab[k] = cnt\n",
        "  cnt += 1\n",
        "\n",
        "print(len(vocab))\n",
        "# print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKhlGiBrdGri",
        "outputId": "b4bdcdfa-800a-48ca-ba35-7dbdfa22b5bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаляем слова, которые слишком редко встречались, это будет основной словарь для обучения"
      ],
      "metadata": {
        "id": "QujHybOaecOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mini_vocab = {}\n",
        "\n",
        "for k, v in vocab.items():\n",
        "  if v < 10000:\n",
        "    mini_vocab[k] = v;\n",
        "\n",
        "print(len(mini_vocab))\n",
        "# print(mini_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQOEX016daYt",
        "outputId": "10089229-7487-4b26-a21f-95e30ea314f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Энкодер и Декодер"
      ],
      "metadata": {
        "id": "-azhfLyqesD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EncodeTokens(ListTokens, VocabTokens): # токены -> числа\n",
        "  res = []\n",
        "  res = [VocabTokens.get(word, VocabTokens['<UNKW>']) for word in ListTokens]\n",
        "  return [VocabTokens['<START>']] + res\n",
        "\n",
        "def DecodeTokens(EncodedTokens, VocabTokens): # числа -> токены\n",
        "  res = []\n",
        "  for i in EncodedTokens:\n",
        "    for word, ind in VocabTokens.items():\n",
        "      if i == ind:\n",
        "        res.append(word)\n",
        "        break\n",
        "  return res"
      ],
      "metadata": {
        "id": "bOxHkRr6e2gD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# применяем к данным\n",
        "\n",
        "train_df['description_encoded'] = train_df['description_tokenized'].apply(lambda x: EncodeTokens(x, mini_vocab))\n",
        "test_df['description_encoded'] = test_df['description_tokenized'].apply(lambda x: EncodeTokens(x, mini_vocab))"
      ],
      "metadata": {
        "id": "b4CQ62-UfMEM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Переведем в нужные нам типы данных, посчитаем описательные статистики и выведем график"
      ],
      "metadata": {
        "id": "9atzgFDzfb_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_df.description_encoded.to_numpy()\n",
        "train_label = pd.get_dummies(train_df['genres']).values\n",
        "\n",
        "test_data = test_df.description_encoded.to_numpy()\n",
        "test_label = pd.get_dummies(test_df['genres']).values"
      ],
      "metadata": {
        "id": "aBkVDVS4fRyB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['description_len'] = train_df['description_encoded'].apply (len)\n",
        "\n",
        "print ('минимальная длина описания:', train_df.description_len.min())\n",
        "print ('средняя длина описания:', round(train_df.description_len.mean()))\n",
        "print ('максимальная длина описания:', train_df.description_len.max())\n",
        "\n",
        "plt.hist(train_df.description_len, density = True)"
      ],
      "metadata": {
        "id": "eLqDFUeHfmXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение"
      ],
      "metadata": {
        "id": "ccAYvFR4fvnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проведем preprocessing данных. Мы добиваем неважными нулями тензоры, потому что иначе модель это не скушает"
      ],
      "metadata": {
        "id": "KMZK3biwf09d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 70\n",
        "\n",
        "train_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    train_data,\n",
        "    value= vocab['<PAD>'],\n",
        "    padding= 'post',\n",
        "    maxlen= MAX_SEQ_LEN)\n",
        "\n",
        "test_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    test_data,\n",
        "    value= vocab['<PAD>'],\n",
        "    padding= 'post',\n",
        "    maxlen= MAX_SEQ_LEN)\n",
        "\n",
        "#print('Тернировочные данные:')\n",
        "#print(train_data.shape)\n",
        "#print(train_data[0])\n",
        "#print()\n",
        "#print('Тестовые данные:')\n",
        "#print(test_data.shape)\n",
        "#print(test_data[0])"
      ],
      "metadata": {
        "id": "3o-kUS3yf0o3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разбиваем на тренировочную и тестовые выборки"
      ],
      "metadata": {
        "id": "y-u8vJpGgxkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "partial_x_train, x_val, partial_y_train, y_val = train_test_split(train_data, train_label,\n",
        "                                                                  test_size = 0.10, random_state = 42)"
      ],
      "metadata": {
        "id": "D3hJtRU_gup1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем объект класса и прописываем архитектуру"
      ],
      "metadata": {
        "id": "DMyTJoL2g3h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(mini_vocab)\n",
        "EMB_SIZE = 32\n",
        "CLASS_NUM = y_val.shape[1]\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(EMB_SIZE, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(EMB_SIZE, return_sequences=True, dropout=0.2, recurrent_dropout=0.1)),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(EMB_SIZE, return_sequences=True, dropout=0.2, recurrent_dropout=0.1)),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(EMB_SIZE, return_sequences=False, dropout=0.2, recurrent_dropout=0.1)),\n",
        "    tf.keras.layers.Dense(CLASS_NUM, activation= 'softmax'),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "VTp9qEEUgBwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начинаем самое обучение модели"
      ],
      "metadata": {
        "id": "kmUfv3y2hI50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "# сохраняем файл с весами и параметрами модели\n",
        "cpt_path = '/content/model_history.hdf5'\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(cpt_path, monitor='acc', verbose=1, save_best_only= True, mode='max')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# history = model.fit(partial_x_train, partial_y_train, validation_data= (x_val, y_val),\n",
        "                   #epochs= NUM_EPOCHS, batch_size= BATCH_SIZE, verbose= 1,\n",
        "                   #callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "OHa75mfshCAh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two hour later...\n",
        "\n",
        "Выводим хорошие (нет) графики loss и accuracy"
      ],
      "metadata": {
        "id": "w9cXtx-ahq-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, history.history['loss'], 'bo', label='Training loss')\n",
        "plt.plot(epochs, history.history['val_loss'], 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, history.history['acc'], 'bo', label='Training mse')\n",
        "plt.plot(epochs, history.history['val_acc'], 'b', label='Validation mse')\n",
        "plt.title('Training and validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "kGBVmBPZhnTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тестирование"
      ],
      "metadata": {
        "id": "NwC1q8i7h81z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# наши 10 классов\n",
        "labels = train_df.genres.unique()\n",
        "\n",
        "# модель, что получилась после обучения\n",
        "my_model = load_model('/content/14_text_classifier.hdf5')"
      ],
      "metadata": {
        "id": "-zhHHXpBhzJO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вводим наш запрос - описание аниме, у которого хотим определить жанр\n",
        "prompt = '12345678'\n",
        "\n",
        "# токенизируем\n",
        "prompt_tokenize = tokenize_text(prompt)\n",
        "\n",
        "# удаляем лишние слова, которые не содержаться в словаре mini_vocab\n",
        "prompt_extra = []\n",
        "for i in prompt_tokenize:\n",
        "  if i in mini_vocab:\n",
        "    prompt_extra.append(i)\n",
        "\n",
        "# энкодим (переводим в числа)\n",
        "prompt_encode = EncodeTokens(prompt_extra, mini_vocab)\n",
        "\n",
        "# получаем предсказание в числах с помощью модели\n",
        "prediction = my_model.predict(tf.constant([prompt_encode]))\n",
        "\n",
        "# переводим на человеческий язык\n",
        "ans = labels[np.argmax(prediction)]\n",
        "ans"
      ],
      "metadata": {
        "id": "A0OljvsliOPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуем в виде функции"
      ],
      "metadata": {
        "id": "zgPtk1CXkiWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ans(prompt):\n",
        "    prompt_tokenize = tokenize_text(prompt)\n",
        "\n",
        "    prompt_extra = []\n",
        "    for i in prompt_tokenize:\n",
        "      if i in mini_vocab:\n",
        "        prompt_extra.append(i)\n",
        "\n",
        "    prompt_encode = EncodeTokens(prompt_extra, mini_vocab)\n",
        "    prediction = my_model.predict(tf.constant([prompt_encode]))\n",
        "\n",
        "    return labels[np.argmax(prediction)]"
      ],
      "metadata": {
        "id": "DqKE3LFJjd2W"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}